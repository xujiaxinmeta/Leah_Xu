{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MinHahDocuments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1niTlxFyYu6AOMEREqxwaHVcqUZSlfkr1",
      "authorship_tag": "ABX9TyPlFWGI7QRUmZJD/HK0zArO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xujiaxinmeta/Leah_Xu/blob/main/MinHahDocuments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similar document searching via MinHash and Locality Sensitive Hashing\n",
        "本练习的目标是了解 minhash 和 LSH 系统如何有效和高效地识别这些实例。"
      ],
      "metadata": {
        "id": "dwUx0yuMCB7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Part I: Preliminaries***\n",
        "**Part IA: Dataset parsing**"
      ],
      "metadata": {
        "id": "Fe8ZDXQVCtnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是生成的 MinHash 签名中的组件数。\n",
        "\n",
        "对应的也是随机哈希函数的个数\n",
        "\n",
        "我们需要计算 MinHash。"
      ],
      "metadata": {
        "id": "kbGunKhVlKwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import time\n",
        "import binascii\n",
        "import random\n",
        "from bisect import bisect_right\n",
        "from heapq import heappop, heappush\n",
        "from __future__ import division\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZZ7papkt1Op",
        "outputId": "4371b9b9-32e2-4d91-8c8a-2b9884a13200"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMOy6qWyByMj"
      },
      "outputs": [],
      "source": [
        "numHashes = 10;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numDocs = 1000"
      ],
      "metadata": {
        "id": "15Ec3eENne5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataFile = \"/content/drive/MyDrive/Colab Notebooks/DataMining/data/articles_\" + str(numDocs) + \".train\"\n",
        "truthFile = \"/content/drive/MyDrive/Colab Notebooks/DataMining/data/articles_\" + str(numDocs) + \".truth\""
      ],
      "metadata": {
        "id": "tjjzqDV6nosH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plagiaries = {}\n",
        "f = open(truthFile, \"r\")\n"
      ],
      "metadata": {
        "id": "-v4CokKen2J7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each line of the files...\n",
        "for line in f:\n",
        "  \n",
        "  # Strip the newline character, if present.\n",
        "  if line[-1] == '\\n':\n",
        "      line = line[0:-1]\n",
        "      \n",
        "  docs = line.split(\" \")"
      ],
      "metadata": {
        "id": "xuIuRBUEn2RC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the two documents to each other.\n",
        "plagiaries[docs[0]] = docs[1]\n",
        "plagiaries[docs[1]] = docs[0]"
      ],
      "metadata": {
        "id": "XSBNEn9dwu8p"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Documents To Sets of Shingles\n",
        "The current shingle ID value to assign to the next new shingle we ,encounter. When a shingle gets added to the dictionary, we'll increment this value."
      ],
      "metadata": {
        "id": "JOzZLH7Sw5HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curShingleID = 0"
      ],
      "metadata": {
        "id": "EQDmiSoPw4jW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of the articles, mapping the article identifier (e.g., \n",
        "# \"t8470\") to the list of shingle IDs that appear in the document.\n",
        "docsAsShingleSets = {}\n",
        "# Open the data file.\n",
        "f = open(dataFile, \"r\")\n",
        "docNames = []\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "totalShingles = 0\n",
        "for i in range(0, numDocs):\n",
        "  \n",
        "  # Read all of the words (they are all on one line) and split them by white\n",
        "  # space.\n",
        "  words = f.readline().split(\" \")\n",
        "   # Retrieve the article ID, which is the first word on the line.  \n",
        "  docID = words[0]\n",
        "  # Maintain a list of all document IDs.  \n",
        "  docNames.append(docID)\n",
        "  del words[0] \n",
        "# 'shinglesInDoc' will hold all of the unique shingle IDs present in the \n",
        "  # current document. If a shingle ID occurs multiple times in the document,\n",
        "  # it will only appear once in the set (this is a property of Python sets).\n",
        "  #重复的单词在集合中只会出现一次\n",
        "  shinglesInDoc = set()\n",
        "  # For each word in the document...\n",
        "  for index in range(0, len(words) - 2):\n",
        "\n",
        "    # Construct the shingle text by combining three words together.\n",
        "    shingle = (words[index] + \" \" + words[index + 1] + \" \" + words[index + 2]).encode()\n",
        "     # Hash the shingle to a 32-bit integer.\n",
        "    \n",
        "    crc = binascii.crc32(shingle) & 0xffffffff\n",
        "    \n",
        "     # Add the hash value to the list of shingles for the current document. \n",
        "    # Note that set objects will only add the value to the set if the set \n",
        "    # doesn't already contain it. \n",
        "    shinglesInDoc.add(crc)\n",
        "    # Store the completed list of shingles for this document in the dictionary.\n",
        "  docsAsShingleSets[docID] = shinglesInDoc\n",
        "  # Count the number of shingles across all documents.\n",
        "  totalShingles = totalShingles + (len(words) - 2)\n",
        "  # Close the data file.  \n",
        "f.close()  \n",
        "print ('\\nShingling ' + str(numDocs) + ' docs took %.2f sec.' % (time.time() - t0))\n",
        " \n",
        "print ('\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVkqchs0xYnL",
        "outputId": "e437bd40-9827-4b90-909a-750d7cef63c5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shingling 1000 docs took 0.30 sec.\n",
            "\n",
            "Average shingles per doc: 251.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Triangle Matrices"
      ],
      "metadata": {
        "id": "BTf2x6jE5CE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define virtual Triangle matrices to hold the similarity values. For storing\n",
        "# similarities between pairs, we only need roughly half the elements of a full\n",
        "# matrix. Using a triangle matrix requires less than half the memory of a full\n",
        "# matrix, and can protect the programmer from inadvertently accessing one of\n",
        "# the empty/invalid cells of a full matrix.\n",
        "# Calculate the number of elements needed in our triangle matrix\n",
        "numElems = int(numDocs * (numDocs - 1) / 2)\n",
        "# Initialize two empty lists to store the similarity values. \n",
        "# 'JSim' will be for the actual Jaccard Similarity values. \n",
        "# 'estJSim' will be for the estimated Jaccard Similarities found by comparing\n",
        "# the MinHash signatures.\n",
        "JSim = [0 for x in range(numElems)]\n",
        "estJSim = [0 for x in range(numElems)]\n",
        "# Define a function to map a 2D matrix coordinate into a 1D index.\n",
        "def getTriangleIndex(i, j):\n",
        "  # If i == j that's an error.\n",
        "  if i == j:\n",
        "    sys.stderr.write(\"Can't access triangle matrix with i == j\")\n",
        "    sys.exit(1)\n",
        "  # If j < i just swap the values.\n",
        "  if j < i:\n",
        "    temp = i\n",
        "    i = j\n",
        "    j = temp\n",
        "    # Calculate the index within the triangular array.\n",
        "  # This fancy indexing scheme is taken from pg. 211 of:\n",
        "  # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf\n",
        "  # But I adapted it for a 0-based index.\n",
        "  # Note: The division by two should not truncate, it\n",
        "  #       needs to be a float. \n",
        "  k = int(i * (numDocs - (i + 1) / 2.0) + j - i) - 1\n",
        "  \n",
        "  return k"
      ],
      "metadata": {
        "id": "FYPYdZpU49tL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Jaccard similarities gets really slow for large numbers of documents.\n",
        "\n",
        "if numDocs <= 2500:\n",
        "#if True:\n",
        "    print (\"\\nCalculating Jaccard Similarities...\")\n",
        "\n",
        "    # Time the calculation.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # For every document pair...\n",
        "    for i in range(0, numDocs):\n",
        "      \n",
        "      # Print progress every 100 documents.\n",
        "      if (i % 100) == 0:\n",
        "        print (\"  (\" + str(i) + \" / \" + str(numDocs) + \")\")\n",
        "\n",
        "      # Retrieve the set of shingles for document i.\n",
        "      s1 = docsAsShingleSets[docNames[i]]\n",
        "      \n",
        "      for j in range(i + 1, numDocs):\n",
        "        # Retrieve the set of shingles for document j.\n",
        "        s2 = docsAsShingleSets[docNames[j]]\n",
        "        \n",
        "        # Calculate and store the actual Jaccard similarity.\n",
        "        JSim[getTriangleIndex(i, j)] = (len(s1.intersection(s2)) / len(s1.union(s2)))    \n",
        "\n",
        "    # Calculate the elapsed time (in seconds)\n",
        "    elapsed = (time.time() - t0)\n",
        "        \n",
        "    print (\"\\nCalculating all Jaccard Similarities took %.2fsec\" % elapsed)\n",
        "# Delete the Jaccard Similarities, since it's a pretty big matrix.    \n",
        "del JSim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch-vrNU958yq",
        "outputId": "9172c479-9fdf-464b-85bf-774b214adbd3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Jaccard Similarities...\n",
            "  (0 / 1000)\n",
            "  (100 / 1000)\n",
            "  (200 / 1000)\n",
            "  (300 / 1000)\n",
            "  (400 / 1000)\n",
            "  (500 / 1000)\n",
            "  (600 / 1000)\n",
            "  (700 / 1000)\n",
            "  (800 / 1000)\n",
            "  (900 / 1000)\n",
            "\n",
            "Calculating all Jaccard Similarities took 12.96sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate MinHash Signatures"
      ],
      "metadata": {
        "id": "J6s64Bpr6iMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time this step.\n",
        "t0 = time.time()\n",
        "\n",
        "print ('\\nGenerating random hash functions...')\n",
        "\n",
        "# Record the maximum shingle ID that we assigned.\n",
        "maxShingleID = 2**32-1\n",
        "\n",
        "# We need the next largest prime number above 'maxShingleID'.\n",
        "# I looked this value up here: \n",
        "# http://compoasso.free.fr/primelistweb/page/prime/liste_online_en.php\n",
        "nextPrime = 4294967311\n",
        "\n",
        "\n",
        "# Our random hash function will take the form of:\n",
        "#   h(x) = (a*x + b) % c\n",
        "# Where 'x' is the input value, 'a' and 'b' are random coefficients, and 'c' is\n",
        "# a prime number just greater than maxShingleID.\n",
        "\n",
        "# Generate a list of 'k' random coefficients for the random hash functions,\n",
        "# while ensuring that the same value does not appear multiple times in the \n",
        "# list.\n",
        "def pickRandomCoeffs(k):\n",
        "  # Create a list of 'k' random values.\n",
        "  randList = []\n",
        "  \n",
        "  while k > 0:\n",
        "    # Get a random shingle ID.\n",
        "    randIndex = random.randint(0, maxShingleID) \n",
        "  \n",
        "    # Ensure that each random number is unique.\n",
        "    while randIndex in randList:\n",
        "      randIndex = random.randint(0, maxShingleID) \n",
        "    \n",
        "    # Add the random number to the list.\n",
        "    randList.append(randIndex)\n",
        "    k = k - 1\n",
        "    \n",
        "  return randList"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0j2GzQs6Vkc",
        "outputId": "7f8979cf-d171-4458-967a-b1a637e42938"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating random hash functions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each of the 'numHashes' hash functions, generate a different coefficient 'a' and 'b'.   \n",
        "coeffA = pickRandomCoeffs(numHashes)\n",
        "coeffB = pickRandomCoeffs(numHashes)"
      ],
      "metadata": {
        "id": "0npp5JsD66gI"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\nGenerating MinHash signatures for all documents...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ahQZOiY68_C",
        "outputId": "d8b0f883-f4a1-44d8-be6a-2d23717b2844"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating MinHash signatures for all documents...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signatures = []\n",
        "# Rather than generating a random permutation of all possible shingles, \n",
        "# we'll just hash the IDs of the shingles that are *actually in the document*,\n",
        "# then take the lowest resulting hash code value. This corresponds to the index \n",
        "# of the first shingle that you would have encountered in the random order.\n",
        "# For each document...\n",
        "for docID in docNames:\n",
        "  \n",
        "  # Get the shingle set for this document.\n",
        "  shingleIDSet = docsAsShingleSets[docID]\n",
        "  \n",
        "  # The resulting minhash signature for this document. \n",
        "  signature = []\n",
        "  \n",
        "  # For each of the random hash functions...\n",
        "  for i in range(0, numHashes):\n",
        "    \n",
        "    # For each of the shingles actually in the document, calculate its hash code\n",
        "    # using hash function 'i'. \n",
        "    \n",
        "    # Track the lowest hash ID seen. Initialize 'minHashCode' to be greater than\n",
        "    # the maximum possible value output by the hash.\n",
        "    minHashCode = nextPrime + 1\n",
        "    \n",
        "    # For each shingle in the document...\n",
        "    for shingleID in shingleIDSet:\n",
        "      # Evaluate the hash function.\n",
        "      hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime \n",
        "      \n",
        "      # Track the lowest hash code seen.\n",
        "      if hashCode < minHashCode:\n",
        "        minHashCode = hashCode\n",
        "\n",
        "    # Add the smallest hash code value as component number 'i' of the signature.\n",
        "    signature.append(minHashCode)\n",
        "  \n",
        "  # Store the MinHash signature for this document.\n",
        "  signatures.append(signature)\n",
        "\n",
        "# Calculate the elapsed time (in seconds)\n",
        "elapsed = (time.time() - t0)\n",
        "        \n",
        "print (\"\\nGenerating MinHash signatures took %.2fsec\" % elapsed ) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbFIF1Ro7FNh",
        "outputId": "31afc713-72ed-4673-fac8-3b75662c9ee4"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating MinHash signatures took 117.41sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare All Signatures"
      ],
      "metadata": {
        "id": "SsqXtjGz7WV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('\\nComparing all signatures...') \n",
        "  \n",
        "# Creates a N x N matrix initialized to 0.\n",
        "\n",
        "# Time this step.\n",
        "t0 = time.time()\n",
        "\n",
        "# For each of the test documents...\n",
        "for i in range(0, numDocs):\n",
        "  # Get the MinHash signature for document i.\n",
        "  signature1 = signatures[i]\n",
        "    \n",
        "  # For each of the other test documents...\n",
        "  for j in range(i + 1, numDocs):\n",
        "    \n",
        "    # Get the MinHash signature for document j.\n",
        "    signature2 = signatures[j]\n",
        "    \n",
        "    count = 0\n",
        "    # Count the number of positions in the minhash signature which are equal.\n",
        "    for k in range(0, numHashes):\n",
        "      count = count + (signature1[k] == signature2[k])\n",
        "    \n",
        "    # Record the percentage of positions which matched.    \n",
        "    estJSim[getTriangleIndex(i, j)] = (count / numHashes)\n",
        "\n",
        "# Calculate the elapsed time (in seconds)\n",
        "elapsed = (time.time() - t0)\n",
        "        \n",
        "print (\"\\nComparing MinHash signatures took %.2fsec\" % elapsed)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpsup8nQ7XVp",
        "outputId": "9d58c696-56d0-454d-8b34-cc6942953fe5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparing all signatures...\n",
            "\n",
            "Comparing MinHash signatures took 1.94sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Similar Document Pairs"
      ],
      "metadata": {
        "id": "M5K-25wq7gTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the true positives and false positives.\n",
        "tp = 0\n",
        "fp = 0\n",
        "  \n",
        "threshold = 0.5  \n",
        "print (\"\\nList of Document Pairs with J(d1,d2) more than\", threshold)\n",
        "print (\"Values shown are the estimated Jaccard similarity and the actual\")\n",
        "print (\"Jaccard similarity.\\n\")\n",
        "print (\"                   Est. J   Act. J\")\n",
        "plagiaries = {}\n",
        "# For each of the document pairs...\n",
        "for i in range(0, numDocs):  \n",
        "  for j in range(i + 1, numDocs):\n",
        "    # Retrieve the estimated similarity value for this pair.\n",
        "    estJ = estJSim[getTriangleIndex(i, j)]\n",
        "    \n",
        "    # If the similarity is above the threshold...\n",
        "    if estJ > threshold:\n",
        "    \n",
        "      # Calculate the actual Jaccard similarity for validation.\n",
        "      s1 = docsAsShingleSets[docNames[i]]\n",
        "      s2 = docsAsShingleSets[docNames[j]]\n",
        "      J = (len(s1.intersection(s2)) / len(s1.union(s2)))\n",
        "      \n",
        "      # Print out the match and similarity values with pretty spacing.\n",
        "      print (\"  %5s --> %5s   %.2f     %.2f\" % (docNames[i], docNames[j], estJ, J))\n",
        "      \n",
        "      if docsAsShingleSets[docNames[i]] == docNames[j]:   \n",
        "        tp = tp + 1\n",
        "      else:\n",
        "        fp = fp + 1\n",
        "\n",
        "# Display true positive and false positive counts.\n",
        "print (\"True positives:  \" + str(tp) + \" / \" + str(int(len(plagiaries.keys()) / 2)))\n",
        "print (\"False positives: \" + str(fp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g5M7hIQ7hG1",
        "outputId": "760a0e18-e8cb-4891-fa50-7dc448b6bfee"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "List of Document Pairs with J(d1,d2) more than 0.5\n",
            "Values shown are the estimated Jaccard similarity and the actual\n",
            "Jaccard similarity.\n",
            "\n",
            "                   Est. J   Act. J\n",
            "   t980 --> t2023   1.00     0.98\n",
            "  t1088 --> t5015   1.00     0.98\n",
            "  t1297 --> t4638   1.00     0.98\n",
            "  t1768 --> t5248   1.00     0.98\n",
            "  t1952 --> t3495   1.00     0.98\n",
            "  t2535 --> t8642   1.00     0.98\n",
            "  t2839 --> t9303   1.00     0.98\n",
            "  t2957 --> t7111   1.00     0.98\n",
            "  t3268 --> t7998   1.00     0.98\n",
            "  t3466 --> t7563   1.00     0.98\n",
            "True positives:  0 / 0\n",
            "False positives: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(dataFile, \"r\")\n",
        "words = f.readline().split(\" \")\n",
        "docID = words[0]\n",
        "shingle = (words[0] + \" \" + words[0 + 1] + \" \" + words[0 + 2]).encode()\n",
        "crc = binascii.crc32(shingle) & 0xffffffff\n",
        "#type(shingle)"
      ],
      "metadata": {
        "id": "uz0KW_mx38Vw"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}